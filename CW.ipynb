{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIABETES PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This script preprocessing includes how to check for missing values,as well as replace missing values, categorize, and process non-numeric values (OUTCOME (object))and duplicate values, and outliers and replace unreasonable values (0).**   \n",
    "\n",
    "**The training data were compared with and without scaling.**  \n",
    "\n",
    "**The model uses logistic regression, k-NN, random forest and MLP. PCA and the model were compared with and without scaling, and the confusion matrix was evaluated in each link.**  \n",
    "\n",
    "**The hyperparametric tuning is optimized by grid search and random search, and the optimal model is selected by cross-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import  Essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,roc_auc_score, precision_score,f1_score,plot_roc_curve,plot_roc_curve, plot_confusion_matrix,classification_report\n",
    "from HF_Functions import correlated_map,label_encoder,grab_col_names,cat_summary,detect_outliers\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyse and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset Showing the dataset information\n",
    "df = pd.read_csv(\"datasets/diabetes-dataset.csv\")\n",
    "df.head()# check first 5 rows of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape# check No. of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()#Check feature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of the data set accessed.\n",
    "df.describe([0.10,0.25,0.50,0.75,0.90,0.95,0.99]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checking Missing values and Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Missing values\n",
    "\n",
    "# Does data has some missing values?\n",
    "dataset = df.isnull().sum().sum()\n",
    "if dataset == 0:\n",
    "    print('Data has no missing values')\n",
    "else:\n",
    "    print('Data has missing values')#checks variables have any Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()# recheck \n",
    "'''When we examine NaN values with isnull() in the data set,\n",
    " no records are found; however, too many 0's stand out in the columns such as blood pressure, BMI, skin thickness.\n",
    "  This is illogical, so these values should be treated as missing values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the value of 0 with NAN\n",
    "df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can  check where are missing and (0)NAN values   \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols, num_cols, cat_but_car = grab_col_names(df)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to upper case for ease of input\n",
    "df.columns = [col.upper() for col in df.columns]\n",
    "cat_cols = [x.upper() for x in cat_cols]\n",
    "num_cols = [x.upper() for x in num_cols]\n",
    "cat_but_car = [x.upper() for x in cat_but_car]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "columns = columns.drop(\"OUTCOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"1 .Violent method: we can delete rows with missing values: missing values can be dealt with by deleting rows or columns with null values. \n",
    "The disadvantage is that a large amount of information is lost and the percentage of missing values is too large to be effective.\"\"\" \n",
    "\"\"\"2. We estimated the missing values using the mean/median.\n",
    "Prevents data loss leading to deleted rows or columns and works well on a small data set and is easy to implement. \"\"\"\n",
    "'''To fill in the missing values, \n",
    "we will group the columns with empty values according to OUTCOME \n",
    "and add the median value of the target variable corresponding to the relevant blank value.'''\n",
    "for i in columns:   \n",
    "    #The action of taking the median value for values with a partial characteristic of 0.\n",
    "  df[i] = df[i].fillna(df.groupby(\"OUTCOME\")[i].transform(\"median\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()# some  0 values  has replace to median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()# Checking unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Check duplicated values and Handling duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dplicated=df.duplicated().sum()   # check  dups value in file\n",
    "Dplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    !!! IMPORTANT!!!\n",
    " Because the sample is less than 1000 after removing the duplicates values and to get a better model,\n",
    " only the code is shown here without removing the duplicate values'''\n",
    "#df.drop_duplicates()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_to_drop = detect_outliers(df, 2 ,[\"PREGNANCIES\", 'GLUCOSE', 'BLOODPRESSURE', 'SKINTHICKNESS', 'INSULIN', 'BMI', 'DIABETESPEDIGREEFUNCTION', 'AGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[outliers_to_drop] # Show the outliers rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[outliers_to_drop].index, inplace=True)# drop  outlier values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the outliers\n",
    "fig,axes=plt.subplots(figsize = (15,10))\n",
    "sns.boxplot(data=df, ax=axes,width=0.5) #draw the grapg of box plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]\n",
    "len(binary_cols)#Using LabelEncoder, change the binary nominal feature to a binary integer 0 or 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in binary_cols:\n",
    "    label_encoder(df, col)\n",
    "\n",
    "df.head()\n",
    "#replace OUTCOME values to binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.hist(orientation='horizontal', figsize=(25,20)) #Plotting horizontal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sns.pairplot(df, hue ='OUTCOME')# # Distribution of results on each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_map(df, plot=True)\n",
    "'''\n",
    "#If the correlation value is bigger than 0, there is a positive correlation. \n",
    " While the value of one variable increases, the value of the other variable also increases.  \n",
    " When there is equality of Correlation = 0 means no correlation. \n",
    " If the correlation is smaller than 0, there is a negative correlation. While one variable increases, the other variable decreases. \n",
    " When the correlations are examined, there are 2 variables that act as a positive correlation to the Outcome dependent variable. \n",
    " These variables are Glucose. As these increase, Outcome variable increases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(df[\"OUTCOME\"])\n",
    "plt.title(\"Quantity of Diabetes\", size=10)\n",
    "plt.show()#View results histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and training datasets without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection of data sets\n",
    "X = df.iloc[:,:-1].values#allocates the data\n",
    "y = df.iloc[:,-1].values#allocates the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Logistic Regression algorithm\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(X_train, y_train)#Fitting the values of x and y\n",
    "train_accuracy = model_LR.score(X_train, y_train)# Assign the training score \n",
    "test_accuracy = model_LR.score(X_test, y_test)#Assign testing score\n",
    "print(\"Logistic Regression model:\")\n",
    "print(\"Training model accuracy:{:.3f}\".format(train_accuracy))#Print Training Accuracy\n",
    "print(\"Testing model accuracy :{:.3f}\".format(test_accuracy))#Print Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier()                #knn classifier\n",
    "model_knn.fit(X_train,y_train)#Fitting the values of x and y with the KNN model\n",
    "# Assign the training score \n",
    "train_accuracy = model_knn.score(X_train, y_train)\n",
    "test_accuracy = model_knn.score(X_test, y_test)#Assign testing score\n",
    "print(\" K-NN model:\")\n",
    "print(\"Training model_knn Accuracy:{:.3f}\".format(train_accuracy))#Print Training Accuracy\n",
    "print(\"Testing model_knn Accuracy: {:.3f}\".format(test_accuracy))#Print Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Random Forest Classifier model\n",
    "model_RF = RandomForestClassifier(n_estimators=100,random_state=0)                \n",
    "model_RF.fit(X_train,y_train)#Fitting the values of x and y with the RandomForestClassifier model\n",
    "train_accuracy = model_RF.score(X_train, y_train)# Assign the training score \n",
    "test_accuracy = model_RF.score(X_test, y_test)#Assign testing score\n",
    "print(\" RandomForestClassifier model:\")\n",
    "print(\"Training model_RF Accuracy:{:.3f}\".format(train_accuracy))#Print Training Accuracy\n",
    "print(\"Testing model_RF Accuracy: {:.3f}\".format(test_accuracy))#Print Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPClassifier(random_state=0)    # using MLPClassifier      \n",
    "MLP.fit(X_train,y_train) #Fitting the values of x and y with theMLPClassifier model\n",
    "MLP_train_accuracy = MLP.score(X_train, y_train)# Assign the training score \n",
    "MLP_test_accuracy = MLP.score(X_test, y_test)#Assign testing score\n",
    "print(\"MLPClassifierr model:\")\n",
    "print(\"Training model_tree without scalling Accuracy:{:.3f}\".format(MLP_train_accuracy))#Print Training Accuracy\n",
    "print(\"Testing model_tree without scalling Accuracy: {:.3f}\".format(MLP_test_accuracy))#Print Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].values#assign feature values\n",
    "pca = PCA(n_components = 2)#PCA class instance, the parameter value of the representative component is 2\n",
    "pComp = pca.fit_transform(X)#Fitted data\n",
    "PDF = pd.DataFrame(data = pComp, columns = ['pc1','pc2'])#generates a data frame from the 2 components\n",
    "PCA_df = pd.concat([PDF, df['OUTCOME']], axis = 1)#generates a data frame from the two components and target\n",
    "sns.relplot(data = PCA_df, x='pc1', y = 'pc2', hue = 'OUTCOME')#Plotting the distribution of \"OUTCOME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics and Confusion matrix (without Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_LR = model_LR.predict(X_test)# make a prediction\n",
    "#creates the confusion matrix\n",
    "cfm_LR = confusion_matrix(y_test,y_test_LR)\n",
    "print(\"Logistic Regression model:\")\n",
    "print('confusion matrix:')\n",
    "print(cfm_LR)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_LR),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_LR),# recall rate\n",
    "       precision_score(y_test,y_test_LR),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_LR),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_LR, average='micro')  #same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])#The name of each column and the Score associated with each value\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_LR)\n",
    "print('')\n",
    "print(Report)#print the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_LR, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_knn = model_knn.predict(X_test)# make a prediction\n",
    "#creates the confusion matrix\n",
    "cfm_knn = confusion_matrix(y_test,y_test_knn)\n",
    "print(\"K-NN model:\")\n",
    "print('confusion matrix:')\n",
    "print(cfm_knn)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_knn),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_knn),# recall rate\n",
    "       precision_score(y_test,y_test_knn),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_knn),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_knn, average='micro')#same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_knn)#creates the cReport of the model, which is a report showing the main classification metrics.\n",
    "print('')\n",
    "print(Report)#prints the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_knn, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_MLP = MLP.predict(X_test)# make a prediction\n",
    "#creates the confusion matrix\n",
    "cm_MLP = confusion_matrix(y_test,y_test_MLP)\n",
    "print(\"MLPClassifier model: \")\n",
    "print('confusion matrix:')\n",
    "print(cm_MLP)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_MLP),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_MLP),# recall rate\n",
    "       precision_score(y_test,y_test_MLP),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_MLP),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_MLP, average='micro')#same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_MLP)#creates the cReport of the model, which is a report showing the main classification metrics.\n",
    "print('')\n",
    "print(Report)#prints the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cm_MLP, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_RF = model_RF.predict(X_test)# make a prediction\n",
    "#creates the confusion matrix\n",
    "cm_RF = confusion_matrix(y_test,y_test_RF)\n",
    "print(\"RandomForestClassifier  model: \")\n",
    "print('confusion matrix:')\n",
    "print(cm_RF)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_RF),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_RF),# recall rate\n",
    "       precision_score(y_test,y_test_RF),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_RF),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_RF, average='micro')#same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_RF)#creates the cReport of the model, which is a report showing the main classification metrics.\n",
    "print('')\n",
    "print(Report)#prints the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cm_RF, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_roc_curve(model_LR, X_test, y_test)\n",
    "plot_roc_curve(model_knn,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(MLP,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(model_RF, X_test, y_test,ax = disp.ax_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training datasets with scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].values#data selectedand  give a  pandas series \n",
    "\n",
    "scaler = StandardScaler()#Pre-processed data\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])#Fit model with scaling features\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = 0)#Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Logistic Regression algorithm\n",
    "model_LRSC= LogisticRegression(C=100,max_iter=1000)#Use logistic regression and max_iter to 1000\n",
    "model_LRSC.fit(X_train, y_train)#Fit model with scaling features\n",
    "\n",
    "LRSC_train_accuracy = model_LRSC.score(X_train, y_train)# Assign the training score \n",
    "LRSC_test_accuracy = model_LRSC.score(X_test, y_test)#Assign testing score\n",
    "print(\"Logistic Regression model:\")\n",
    "print(\"Training model with scalling accuracy:{:.3f}\".format(LRSC_train_accuracy))#Print Training Accuracy\n",
    "print(\"Testing model with scalling accuracy :{:.3f}\".format(LRSC_test_accuracy))#Print Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using K-NN algorithm\n",
    "model_knnSC = KNeighborsClassifier()       \n",
    "model_knnSC.fit(X_train,y_train)#Fitting the values of x and y with the KNN model\n",
    "# Assign the training score \n",
    "knnSC_train_accuracy = model_knnSC.score(X_train, y_train)\n",
    "knnSC_test_accuracy = model_knnSC.score(X_test, y_test)#Assign testing score\n",
    "print(\" K-NN model:\")\n",
    "print(\" Training model_knn with scalling Accuracy:{:.3f}\".format(knnSC_train_accuracy))#Print Training Accuracy\n",
    "print(\" Testing model_knn with scalling Accuracy: {:.3f}\".format(knnSC_test_accuracy))#Print Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPSC = MLPClassifier(random_state=0)    # using MLPClassifier      \n",
    "MLPSC.fit(X_train,y_train) #Fitting the values of x and y with theMLPClassifier model\n",
    "MLPSC_train_accuracy = MLPSC.score(X_train, y_train)# Assign the training score \n",
    "MLPSC_test_accuracy = MLPSC.score(X_test, y_test)#Assign testing score\n",
    "print(\"MLPClassifierr model:\")\n",
    "print(\"Training model_tree with scalling Accuracy:{:.3f}\".format(MLPSC_train_accuracy))#Print Training Accuracy\n",
    "print(\"Testing model_tree with scalling Accuracy: {:.3f}\".format(MLPSC_test_accuracy))#Print Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Random Forest Classifier model\n",
    "model_RFSC = RandomForestClassifier()                \n",
    "model_RFSC.fit(X_train,y_train)#Fitting the values of x and y with the RandomForestClassifier model\n",
    "\n",
    "RFSC_train_accuracy = model_RFSC.score(X_train, y_train)# Assign the training score \n",
    "RFSC_test_accuracy = model_RFSC.score(X_test, y_test)#Assign testing score\n",
    "print(\"Random Forest Classifier model:\")\n",
    "print(\"Training model_RF with scalling Accuracy:{:.3f}\".format(RFSC_train_accuracy))#Print Training Accuracy\n",
    "print(\"Testing model_RF with scalling Accuracy: {:.3f}\".format(RFSC_test_accuracy))#Print Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].values#assign feature values\n",
    "pca = PCA(n_components = 2)#PCA class instance, the parameter value of the representative component is 2\n",
    "pComp = pca.fit_transform(X)#Fitted data\n",
    "PDF = pd.DataFrame(data = pComp, columns = ['pc1','pc2'])#generates a data frame from the 2 components\n",
    "PCA_df = pd.concat([PDF, df['OUTCOME']], axis = 1)#generates a data frame from the two components and target\n",
    "sns.relplot(data = PCA_df, x='pc1', y = 'pc2', hue = 'OUTCOME')#Plotting the distribution of \"OUTCOME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics and Confusion matrix (Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_LRSC = model_LRSC.predict(X_test)# make a prediction\n",
    "#creates the confusion matrix\n",
    "cfm_LRSC = confusion_matrix(y_test,y_test_LRSC)\n",
    "print(\"Logistic Regression model:\")\n",
    "print('confusion matrix:')\n",
    "print(cfm_LRSC)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_LRSC),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_LRSC),# recall rate\n",
    "       precision_score(y_test,y_test_LRSC),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_LRSC),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_LRSC, average='micro')  #same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])#The name of each column and the Score associated with each value\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_LRSC)\n",
    "print('')\n",
    "print(Report)#print the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_LRSC, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_knnSC = model_knnSC.predict(X_test)# make a prediction\n",
    "#creates the confusion matrix\n",
    "cfm_KNNSC = confusion_matrix(y_test,y_test_knnSC)\n",
    "print(\"K-NN model:\")\n",
    "print('confusion matrix:')\n",
    "print(cfm_KNNSC)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_knnSC),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_knnSC),# recall rate\n",
    "       precision_score(y_test,y_test_knnSC),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_knnSC),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_knnSC, average='micro')#same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_knnSC)#creates the cReport of the model, which is a report showing the main classification metrics.\n",
    "print('')\n",
    "print(Report)#prints the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_KNNSC, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_MLPSC = MLPSC.predict(X_test)# make a prediction\n",
    "#creates the confusion matrix\n",
    "cfm_MLPSC = confusion_matrix(y_test,y_test_MLPSC)\n",
    "print(\"MLPClassifier model: \")\n",
    "print('confusion matrix:')\n",
    "print(cfm_MLPSC)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_MLPSC),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_MLPSC),# recall rate\n",
    "       precision_score(y_test,y_test_MLPSC),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_MLPSC),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_MLPSC, average='micro')#same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_MLPSC)#creates the cReport of the model, which is a report showing the main classification metrics.\n",
    "print('')\n",
    "print(Report)#prints the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_MLPSC, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_RFSC = model_RFSC.predict(X_test)# make a prediction\n",
    "#creates the confusion matrix\n",
    "cfm_RFSC = confusion_matrix(y_test,y_test_RFSC)\n",
    "print(\"RandomForestClassifier  model: \")\n",
    "print('confusion matrix:')\n",
    "print(cfm_RFSC)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_RFSC),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_RFSC),# recall rate\n",
    "       precision_score(y_test,y_test_RFSC),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_RFSC),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_RFSC, average='micro')#same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_RFSC)#creates the cReport of the model, which is a report showing the main classification metrics.\n",
    "print('')\n",
    "print(Report)#prints the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_RFSC, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning and Optimisation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()                      #Solution help by Mark Elshaw \n",
    "LR_Dic=dict()# set dic and Getting indexes of values per hyper-parameter\n",
    "#Add Hyperparametrs\n",
    "LR_Dic['multi_class']= ['auto','ovr','multionmial']\n",
    "LR_Dic['solver']=['liblinear', 'saga','newton-cg', 'lbfgs', 'sag',]\n",
    "LR_Dic['penalty']= ['l1','l2','elasticnet','none']\n",
    "# LR_Dic['c_values']= [100, 10, 1.0, 0.1, 0.01, 0.001]  \n",
    "print(LR_Dic)#prints dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!  Dr. Trang Doan week6 solution\n",
    "\n",
    "grid_LR = GridSearchCV(model, LR_Dic, cv=5, scoring = \"accuracy\", return_train_score = False)# Perform a grid search\n",
    "grid_LR.fit(X,y)#Fit instances of Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_LR.best_score_)#print Best Score\n",
    "print(grid_LR.best_params_)#print Best parameter\n",
    "print(grid_LR.best_estimator_)#print estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_LR.cv_results_)[[\"mean_test_score\",\"params\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dr. Trang Doan weekk6 solution\n",
    "\n",
    "rand = RandomizedSearchCV(model, LR_Dic, cv =5, scoring =\"accuracy\", n_iter = 20, random_state =5, return_train_score = False)#performs random search on KNN\n",
    "rand.fit(X,y)#fits the X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand.best_score_)#print Best Score\n",
    "print(rand.best_params_)#print Best para\n",
    "print(rand.best_estimator_)#print estimatormeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimised Logistic Regression  model and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_OP = LogisticRegression(C=100,multi_class='auto',penalty='l2',max_iter=1000,solver='liblinear',)\n",
    "model_OP.fit(X_train, y_train) \n",
    "train_accuracy_OP= model_OP.score(X_train, y_train)\n",
    "test_accuracy_OP = model_OP.score(X_test, y_test)\n",
    "#Cross-validation: evaluating estimator performance\n",
    "scores = cross_val_score(model_OP,X,y,cv = 5,scoring = 'accuracy')\n",
    "print(\"Logistic Regression model:\")\n",
    "print(\"Training model accuracy:{:.3f}\".format(train_accuracy_OP))\n",
    "print(\"Testing model accuracy :{:.3f}\".format(test_accuracy_OP))\n",
    "print(scores)\n",
    "print(\"Max score:{:.3f}\".format(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics and Confusion matrix (Hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_OP = model_OP.predict(X_test)\n",
    "#creates the confusion matrix\n",
    "cfm_LROP = confusion_matrix(y_test,y_test_OP)\n",
    "print(\"Logistic Regression model:\")\n",
    "print('confusion matrix:')\n",
    "print(cfm_LROP)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_OP),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_OP),# recall rate\n",
    "       precision_score(y_test,y_test_OP),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_OP),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_OP, average='micro')  #same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])#The name of each column and the Score associated with each value\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_OP)\n",
    "print('')\n",
    "print(Report)#print the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_LROP, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning for KNN\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train,y_train)#fit the model using X_train as training data and Y_train as target values\n",
    "k_range = list(range(1,31))# set n_neighbors range\n",
    "weights_options = ['uniform','distance']\n",
    "param_grid = dict(n_neighbors = k_range, weights = weights_options)\n",
    "print(param_grid)#prints dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!  Dr. Trang Doan week6 solution\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring = \"accuracy\", return_train_score = False)#instance of Grid Search\n",
    "grid.fit(X,y)#fit the instance of gridsearch using X as training data and Y as target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)#print Best Score\n",
    "print(grid.best_params_)#print Best parameter\n",
    "print(grid.best_estimator_)#print estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_)[[\"mean_test_score\",\"params\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = RandomizedSearchCV(model, param_grid, cv =5, scoring = \"accuracy\", n_iter = 20, random_state =5, return_train_score = False)#performs random search on KNN\n",
    "rand.fit(X,y)#fits the X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand.best_score_)#print Best Score\n",
    "print(rand.best_params_)#print Best para\n",
    "print(rand.best_estimator_)#print estimatormeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimised Logistic Regression  model and cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using K-NN algorithm\n",
    "model_knnOP = KNeighborsClassifier(n_neighbors=15)                #knn classifier\n",
    "model_knnOP.fit(X_train,y_train)\n",
    "#Cross-validation: evaluating estimator performance\n",
    "scores = cross_val_score(model_knnOP,X,y,cv = 5,scoring = 'accuracy')\n",
    "knnOP_train_accuracy = model_knnOP.score(X_train, y_train)\n",
    "knnOP_test_accuracy = model_knnOP.score(X_test, y_test)\n",
    "print(\" K-NN model:\")\n",
    "print(scores)\n",
    "print(\" Training model_knn with scalling Accuracy:{:.3f}\".format(knnOP_train_accuracy))\n",
    "print(\" Testing model_knn withing Accuracy: {:.3f}\".format(knnOP_test_accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics and Confusion matrix (Hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_knnOP = model_knnOP.predict(X_test)\n",
    "#creates the confusion matrix\n",
    "cfm_KNNOP = confusion_matrix(y_test,y_test_knnOP)\n",
    "print(\"K-NN model:\")\n",
    "print('confusion matrix:')\n",
    "print(cfm_KNNOP)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_knnOP),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_knnOP),# recall rate\n",
    "       precision_score(y_test,y_test_knnOP),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_knnOP),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_knnOP, average='micro')#same like the accuracy_score\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_knnOP)#creates the cReport of the model, which is a report showing the main classification metrics.\n",
    "print('')\n",
    "print(Report)#prints the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_KNNOP, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)#hyperparameter tuning for RF\n",
    "model.fit(X_train,y_train)#fit the model using X_train as training data and Y_train as target values\n",
    "RF_DIC=dict()\n",
    "RF_DIC['class_weight']=['balanced', 'balanced_subsample']\n",
    "RF_DIC['criterion'] = ['gini','entropy']\n",
    "RF_DIC['max_features']=['auto', 'sqrt','log2']\n",
    "RF_DIC['max_leaf_nodes'] = range(2,10)\n",
    "RF_DIC['min_samples_split'] = [2,3,4]\n",
    "RF_DIC['min_samples_leaf'] = [1, 3, 5]\n",
    "RF_DIC['max_depth'] = [3, 6, 10, None]\n",
    "RF_DIC['n_estimators']= [100, 500, 700]\n",
    "print(RF_DIC)#prints dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!  Dr. Trang Doan week6 solution\n",
    "grid = GridSearchCV(model, RF_DIC, cv=5, scoring = \"accuracy\", return_train_score = False)#instance of Grid Search\n",
    "grid.fit(X,y)#fit the instance of gridsearch using X as training data and Y as target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)#print Best Score\n",
    "print(grid.best_params_)#print Best parameter\n",
    "print(grid.best_estimator_)#print estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!  Dr. Trang Doan week6 solution\n",
    "rand = RandomizedSearchCV(model, RF_DIC, cv =5, scoring = \"accuracy\", n_iter = 20, random_state =5, return_train_score = False)#performs random search on KNN\n",
    "rand.fit(X,y)#fits the X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand.best_score_)#print Best Score\n",
    "print(rand.best_params_)#print Best para\n",
    "print(rand.best_estimator_)#print estimatormeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimised RandomForestClassifier model and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Random Forest Classifier model\n",
    "model_RFOP = RandomForestClassifier(class_weight='balanced',criterion='entropy',max_depth=9, max_features='log2',n_estimators=100,random_state=0)                \n",
    "model_RFOP.fit(X_train,y_train)\n",
    "#Cross-validation: evaluating estimator performance\n",
    "scores = cross_val_score(model_RFOP,X,y,cv=5,scoring='accuracy')\n",
    "train_accuracy = model_RFOP.score(X_train, y_train)\n",
    "test_accuracy = model_RFOP.score(X_test, y_test)\n",
    "print(scores)\n",
    "print(\"Training model_RFOP Accuracy:{:.3f}\".format(train_accuracy))\n",
    "print(\"Testing model_RFOP Accuracy: {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics and Confusion matrix (Hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_RFOP = model_RFOP.predict(X_test)\n",
    "\n",
    "cfm_RFOP = confusion_matrix(y_test,y_test_RFOP)#creates the confusion matrix\n",
    "print(\"Logistic Regression model:\")\n",
    "print('confusion matrix:')\n",
    "print(cfm_RFOP)\n",
    "print('Evaluation metrics:')\n",
    "#evaluation metrics for the model\n",
    "#creates a dataframe which contains the value of the accuracy, recale, precision and roc_auc score\n",
    "PDF = pd.DataFrame(data=\n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_RFOP),\n",
    "       recall_score(y_test,y_test_RFOP),\n",
    "       precision_score(y_test,y_test_RFOP),\n",
    "       roc_auc_score(y_test,y_test_RFOP),\n",
    "       f1_score(y_test, y_test_RFOP, average='micro')\n",
    "       ]\n",
    "],\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])\n",
    "\n",
    "#creates a table with the accuracy, recall, precision and roc_auc scores\n",
    "labels = ['Probability of NOT having diabetes','Probability of having diabetes']#target names for classification report\n",
    "Report =classification_report(y_test, y_test_RFOP, target_names=labels)#creates the cReport of the model, which is a report showing the main classification metrics.\n",
    "print(Report)#prints the classification_report\n",
    "print(PDF)#prints the df with the score of the evaluation metrics\n",
    "\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_RFOP, cmap=\"GnBu\",annot=True,fmt=\".0f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()    \n",
    "MLP_DIC=dict()# set dic and Getting indexes of values per hyper-parameter\n",
    "MLP_DIC['activation']=['identity', 'logistic', 'tanh', 'relu'] # set \n",
    "MLP_DIC['solver']=[ 'lbfgs', 'sgd','adam']\n",
    "MLP_DIC['learning_rate']=['constant','invscaling','adaptive']\n",
    "print(MLP_DIC) #prints dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!  Dr. Trang Doan week6 solution\n",
    "grid = GridSearchCV(model, MLP_DIC, cv=5, scoring = \"accuracy\", return_train_score = False)#instance of Grid Search\n",
    "grid.fit(X,y)#fit the instance of gridsearch using X as training data and Y as target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)#print Best Score\n",
    "print(grid.best_params_)#print Best parameter\n",
    "print(grid.best_estimator_)#print estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!  Dr. Trang Doan week6 solution\n",
    "rand = RandomizedSearchCV(model, MLP_DIC, cv =5, scoring = \"accuracy\", n_iter = 20, random_state =5, return_train_score = False)#performs random search on KNN\n",
    "rand.fit(X,y)#fits the X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand.best_score_)#print Best Score\n",
    "print(rand.best_params_)#print Best para\n",
    "print(rand.best_estimator_)#print estimatormeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimised RandomForestClassifier model and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLPClassifier\n",
    "MLP_OP = MLPClassifier(activation='tanh', learning_rate='invscaling', solver='lbfgs',random_state=0,max_iter=1000,alpha=1)            \n",
    "MLP_OP .fit(X_train,y_train)\n",
    "#Cross-validation: evaluating estimator performance\n",
    "scores = cross_val_score(MLP_OP,X,y,cv = 5,scoring = 'accuracy')\n",
    "MLPOP_train_accuracy = MLP_OP.score(X_train, y_train)\n",
    "MLPOP_test_accuracy = MLP_OP .score(X_test, y_test)\n",
    "print(\"MLPClassifierr model:\")\n",
    "print(scores)\n",
    "print(\"Training MLP_OP  with scalling Accuracy:{:.3f}\".format(MLPOP_train_accuracy))\n",
    "print(\"Testing MLP_OP  with scalling Accuracy: {:.3f}\".format(MLPOP_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics and Confusion matrix (Hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Evaluation metrics\n",
    "y_test_MLPOP = MLP_OP.predict(X_test)\n",
    "#creates the confusion matrix\n",
    "cfm_MLPOP = confusion_matrix(y_test,y_test_MLPOP)\n",
    "print(\"MLPClassifier model:\")\n",
    "print('confusion matrix:')\n",
    "print(cfm_LROP)#print confusion matrix result\n",
    "print('Evaluation metrics:')#evaluation metrics for the model\n",
    "PDF = pd.DataFrame(data=#creates a dataframe \n",
    "[\n",
    "       [\n",
    "       accuracy_score(y_test,y_test_MLPOP),#Accuracy classification score\n",
    "       recall_score(y_test,y_test_MLPOP),# recall rate\n",
    "       precision_score(y_test,y_test_MLPOP),#Precision rate\n",
    "       roc_auc_score(y_test,y_test_MLPOP),#Predicted Receiver Operating Characteristic Curve  (ROC AUC) \n",
    "       f1_score(y_test, y_test_MLPOP, average='micro')  #same like the accuracy_score\n",
    "       ]\n",
    "]\n",
    "       columns=['accuracy','recall','precision','roc_auc_score','f1_score'],index = ['Score'])#The name of each column and the Score associated with each value\n",
    "print(PDF)#prints the PDF with the evaluation metrics' scores\n",
    "Report =classification_report(y_test, y_test_MLPOP)\n",
    "print('')\n",
    "print(Report)#print the classification_report\n",
    "plt.figure(figsize = (10,10))#sets the size of the figure\n",
    "sns.heatmap(data = cfm_MLPOP, cmap=\"GnBu\",annot=True,fmt=\".0f\")#The heatmap contains the values within the PDF and is displayed inside the matrix\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7cd546514abcdaaf9eaba3a8debdefe65d38cd3deeed0c62925429eba44c3a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
